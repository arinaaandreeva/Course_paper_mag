{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":9801075,"sourceType":"datasetVersion","datasetId":6006872},{"sourceId":9806342,"sourceType":"datasetVersion","datasetId":6010899},{"sourceId":10139918,"sourceType":"datasetVersion","datasetId":6258261},{"sourceId":10139922,"sourceType":"datasetVersion","datasetId":6258265},{"sourceId":10253875,"sourceType":"datasetVersion","datasetId":6297065},{"sourceId":10304887,"sourceType":"datasetVersion","datasetId":6378806},{"sourceId":10351700,"sourceType":"datasetVersion","datasetId":6410107},{"sourceId":203900450,"sourceType":"kernelVersion"},{"sourceId":215616115,"sourceType":"kernelVersion"},{"sourceId":216017958,"sourceType":"kernelVersion"},{"sourceId":216577393,"sourceType":"kernelVersion"},{"sourceId":205603,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":175344,"modelId":197693},{"sourceId":220368,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":187942,"modelId":209985},{"sourceId":221815,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":189194,"modelId":211200}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":80.344101,"end_time":"2024-10-26T03:27:42.952247","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-26T03:26:22.608146","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"252dd2de87de42f4becd877fbbafc26b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff55584ae0ab4f39ae471f314eaad988","placeholder":"​","style":"IPY_MODEL_b8d338c473aa4dc3ba25f37a997a9037","value":" 1/1 [00:00&lt;00:00, 33.79it/s]"}},"48a2731fb59b4ce8ace5b53d6f0e3337":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8dbc79c7c5a48318466a102c55801bb","placeholder":"​","style":"IPY_MODEL_ebd06aaaa7024a1684ba1b9fe89358cf","value":"100%"}},"56da652f3eeb42aca986e6fd815629dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bb4e0df01c44716afebab25aafe9f5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4e41234b0cc4f3e9313d971f5aadc9f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6408698650f74dd699bcc914b850e396","value":1}},"6408698650f74dd699bcc914b850e396":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9cb493ec04fc4ed391f5ac28cc84500e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48a2731fb59b4ce8ace5b53d6f0e3337","IPY_MODEL_5bb4e0df01c44716afebab25aafe9f5f","IPY_MODEL_252dd2de87de42f4becd877fbbafc26b"],"layout":"IPY_MODEL_56da652f3eeb42aca986e6fd815629dc"}},"a8dbc79c7c5a48318466a102c55801bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8d338c473aa4dc3ba25f37a997a9037":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebd06aaaa7024a1684ba1b9fe89358cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4e41234b0cc4f3e9313d971f5aadc9f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff55584ae0ab4f39ae471f314eaad988":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rtdl_num_embeddings -q --no-index --find-links=/kaggle/input/jane-street-import/rtdl_num_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:53:32.214198Z","iopub.execute_input":"2025-01-10T09:53:32.214501Z","execution_failed":"2025-01-10T09:53:40.169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, sys, gc\nimport pickle\nimport dill\nimport numpy as np\nimport pandas as pd\nimport polars as pl\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\n\nfrom sklearn.metrics import r2_score\n\nimport torch.optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nimport math\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nfrom tabm_reference import Model, make_parameter_groups\n\nimport warnings\nimport joblib\nfrom pytorch_lightning.callbacks import Callback\nimport gc\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor, Booster\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nsys.path.append(\"/kaggle/input/jane-street-real-time-market-data-forecasting\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-10T09:53:40.169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CONFIG:\n    \"\"\"Configuration class for model parameters\"\"\"\n    seed = 42  # Random seed for reproducibility\n    target_col = \"responder_6\"  # Target variable name\n    # Features: 79 base features + 9 lagged features\n    feature_cols = [f\"feature_{idx:02d}\" for idx in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n    # Paths to pre-trained models\n    model_paths = [\n        \"/kaggle/input/js-xs-nn-trained-model\",  # Neural Network models\n        \"/kaggle/input/js-with-lags-trained-xgb/result.pkl\", # XGBoost model\n        \"/kaggle/input/als-e-106-pp0-40-xgb-5fold/result0.pkl\",\n        \"/kaggle/input/als-e-106-pp0-40-xgb-5fold/result1.pkl\",\n        \"/kaggle/input/als-e-106-pp0-40-xgb-5fold/result2.pkl\",\n        \"/kaggle/input/als-e-106-pp0-40-xgb-5fold/result3.pkl\",\n        \"/kaggle/input/als-e-106-pp0-40-xgb-5fold/result4.pkl\"  \n    ]\n    ae_mlp_cols = [f\"feature_{idx:02d}\" for idx in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)] + ['symbol_id', 'time_id', 'date_id']\n    ae_model_path = '/kaggle/input/ae_mlp_js24_v2/pytorch/ae_mlp_js24_v2/1/ae_mlp_model_06_01_2025.pth'\n    ridge_model_path = \"/kaggle/input/jsridgev01011635\"\n    scaler_path = '/kaggle/input/scaler_new/other/scaler_new/1/robust_scaler_07_01.pkl'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load validation data\nvalid = pl.scan_parquet(f\"/kaggle/input/js24-preprocessing-create-lags/validation.parquet/\").collect().to_pandas()\n\n# Load XGBoost model\nxgb_model = None\nwith open(CONFIG.model_paths[2], \"rb\") as fp:\n    result = pickle.load(fp)\n    xgb_model = result[\"model\"]\nxgb_feature_cols = [\"symbol_id\", \"time_id\"] + CONFIG.feature_cols\n\nxgb_model2 = None\nwith open(CONFIG.model_paths[4], \"rb\") as fp:\n    result = pickle.load(fp)\n    xgb_model2 = result[\"model\"]\n\nxgb_model4 = None\nwith open(CONFIG.model_paths[6], \"rb\") as fp:\n    result = pickle.load(fp)\n    xgb_model4 = result[\"model\"]\n\nxgb_model5 = None\nwith open(CONFIG.model_paths[1], \"rb\") as fp:\n    result = pickle.load(fp)\n    xgb_model5 = result[\"model\"]\n\ndef r2_val(y_true, y_pred, sample_weight):\n    \"\"\"\n    Calculate weighted R² score\n    Args:\n        y_true: True values\n        y_pred: Predicted values\n        sample_weight: Weights for each sample\n    Returns:\n        Weighted R² score\n    \"\"\"\n    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n    return r2\n\nclass NN(LightningModule):\n    \"\"\"Neural Network model using PyTorch Lightning\"\"\"\n    \n    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay):\n        \"\"\"\n        Initialize the neural network\n        Args:\n            input_dim: Input feature dimension\n            hidden_dims: List of hidden layer dimensions\n            dropouts: List of dropout rates\n            lr: Learning rate\n            weight_decay: Weight decay for regularization\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Build network architecture\n        layers = []\n        in_dim = input_dim\n        for i, hidden_dim in enumerate(hidden_dims):\n            layers.append(nn.BatchNorm1d(in_dim))  # Batch normalization\n            if i > 0:\n                layers.append(nn.SiLU())  # SiLU activation (except first layer)\n            if i < len(dropouts):\n                layers.append(nn.Dropout(dropouts[i]))  # Dropout for regularization\n            layers.append(nn.Linear(in_dim, hidden_dim))  # Linear layer\n            in_dim = hidden_dim\n            \n        # Output layer\n        layers.append(nn.Linear(in_dim, 1))\n        layers.append(nn.Tanh())  # Tanh activation for bounded output\n        \n        self.model = nn.Sequential(*layers)\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.validation_step_outputs = []\n\n    def forward(self, x):\n        \"\"\"Forward pass with scaling\"\"\"\n        return 5 * self.model(x).squeeze(-1)  # Scale output to [-5, 5] range\n\n    def training_step(self, batch):\n        \"\"\"Single training step\"\"\"\n        x, y, w = batch\n        y_hat = self(x)\n        loss = F.mse_loss(y_hat, y, reduction='none') * w  # Weighted MSE loss\n        loss = loss.mean()\n        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        return loss\n\n    def validation_step(self, batch):\n        \"\"\"Single validation step\"\"\"\n        x, y, w = batch\n        y_hat = self(x)\n        loss = F.mse_loss(y_hat, y, reduction='none') * w\n        loss = loss.mean()\n        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        self.validation_step_outputs.append((y_hat, y, w))\n        return loss\n\n    def on_validation_epoch_end(self):\n        \"\"\"Compute validation metrics at epoch end\"\"\"\n        if not self.trainer.sanity_checking:\n            y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n            val_r_square = r2_val(y, prob, weights)\n            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n        self.validation_step_outputs.clear()\n\n    def configure_optimizers(self):\n        \"\"\"Configure optimizer and learning rate scheduler\"\"\"\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \n            mode='min', \n            factor=0.5, \n            patience=5, \n            verbose=True\n        )\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss',\n            }\n        }\n\n    def on_train_epoch_end(self):\n        \"\"\"Log metrics at end of training epoch\"\"\"\n        if not self.trainer.sanity_checking:\n            epoch = self.trainer.current_epoch\n            metrics = {k: v.item() if isinstance(v, torch.Tensor) else v \n                      for k, v in self.trainer.logged_metrics.items()}\n            formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n            print(f\"Epoch {epoch}: {formatted_metrics}\")\n\n# Load ensemble of models (5-fold cross-validation)\nN_folds = 5\nmodels = []\nfor fold in range(N_folds):\n    checkpoint_path = f\"{CONFIG.model_paths[0]}/nn_{fold}.model\"\n    model = NN.load_from_checkpoint(checkpoint_path)\n    models.append(model.to(\"cuda:0\"))","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"execution_failed":"2025-01-10T09:53:40.169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clear validation data from memory to free up space\ndel valid\ngc.collect()\n\n# Global variable to store lagged features\nlags_: pl.DataFrame | None = None\n\ndef predict_nn_xgb(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n    \"\"\"\n    Make predictions using ensemble of XGBoost and Neural Network models\n    \n    Args:\n        test: DataFrame containing test data\n        lags: DataFrame containing lagged features (optional)\n        \n    Returns:\n        DataFrame with predictions\n    \"\"\"\n    global lags_\n    \n    # Store lags in global variable if provided\n    if lags is not None:\n        lags_ = lags\n\n    # Initialize predictions DataFrame with row_id and placeholder predictions\n    predictions_nn = test.select('row_id', pl.lit(0.0).alias('responder_6',))\n\n    # Process lagged features\n    # Get last record for each date_id and symbol_id combination\n    lags = lags_.clone().group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last()\n    \n    # Join test data with lagged features\n    test = test.join(lags, on=[\"date_id\", \"symbol_id\"], how=\"left\")\n\n    # Initialize arrays for model predictions\n    preds_xgb = np.zeros((test.shape[0],))  # XGBoost predictions\n    preds_nn = np.zeros((test.shape[0],))   # Neural Network predictions\n\n    # Generate XGBoost predictions\n    preds_xgb += xgb_model.predict(test[xgb_feature_cols].to_pandas()) * 0.25\n    preds_xgb += xgb_model2.predict(test[xgb_feature_cols].to_pandas()) * 0.25\n    preds_xgb += xgb_model4.predict(test[xgb_feature_cols].to_pandas()) * 0.25\n    preds_xgb += xgb_model5.predict(test[xgb_feature_cols].to_pandas()) * 0.25\n\n    # Generate Neural Network predictions\n    # Prepare input data\n    test_input = test[CONFIG.feature_cols].to_pandas()\n    # Handle missing values: forward fill then fill remaining with zeros\n    test_input = test_input.fillna(method='ffill').fillna(0)\n    # Convert to PyTorch tensor and move to GPU\n    test_input = torch.FloatTensor(test_input.values).to(\"cuda:0\")\n\n    # Generate predictions from Neural Network ensemble\n    with torch.no_grad():  # Disable gradient calculation for inference\n        for i, nn_model in enumerate(models):\n            nn_model.eval()  # Set model to evaluation mode\n            # Average predictions from all models\n            preds_nn += nn_model(test_input).cpu().numpy() / len(models)\n\n    # Combine predictions with equal weights (50% XGBoost, 50% Neural Network)\n    preds = 0.55 * preds_xgb + 0.45 * preds_nn\n\n    # Create final predictions DataFrame\n    predictions_nn = test.select('row_id').\\\n        with_columns(\n            pl.Series(\n                name='responder_6',\n                values=np.clip(preds, a_min=-5, a_max=5),  # Clip predictions to [-5, 5] range\n                dtype=pl.Float64,\n            )\n        )\n\n    return predictions_nn","metadata":{"_kg_hide-input":false,"papermill":{"duration":0.031534,"end_time":"2024-10-26T03:27:39.453592","exception":false,"start_time":"2024-10-26T03:27:39.422058","status":"completed"},"tags":[],"_kg_hide-output":false,"trusted":true,"execution":{"execution_failed":"2025-01-10T09:53:40.169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## AE MLP","metadata":{}},{"cell_type":"code","source":"from joblib import load","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-10T09:53:40.170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# AE-MLP with Dropout & L2-regulirization\nclass AE_MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim=128, output_dim=1, dropout_rate=0.3):\n        super(AE_MLP, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),  # Dropout after activation not to overfit\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim // 2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n# Взвешенная Huber Loss\ndef weighted_loss(predictions, targets, weights, delta=1.0):\n    loss = nn.SmoothL1Loss(beta=delta, reduction='none')  # Huber Loss\n    per_sample_loss = loss(predictions, targets)\n    weighted_loss = (per_sample_loss * weights).mean()  # weight loss\n    return weighted_loss\n\n    \nae_model = AE_MLP(input_dim=91, hidden_dim=128) \n\nae_model.load_state_dict(torch.load(CONFIG.ae_model_path, weights_only=True))\nae_model.to('cuda:0')\n\n#ae_model.eval()\nscaler = load(CONFIG.scaler_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-10T09:53:40.170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Global variable to store lagged features\nlags_: pl.DataFrame | None = None\n\ndef predict_nn_ae(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n    \"\"\"\n    Make predictions using ensemble of XGBoost and Neural Network models\n    \n    Args:\n        test: DataFrame containing test data\n        lags: DataFrame containing lagged features (optional)\n        \n    Returns:\n        DataFrame with predictions\n    \"\"\"\n    global lags_\n    \n    # Store lags in global variable if provided\n    if lags is not None:\n        lags_ = lags\n\n    # Initialize predictions DataFrame with row_id and placeholder predictions\n    predictions_nn = test.select('row_id', pl.lit(0.0).alias('responder_6',))\n\n    # Process lagged features\n    # Get last record for each date_id and symbol_id combination\n    lags = lags_.clone().group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last()\n    \n    # Join test data with lagged features\n    test = test.join(lags, on=[\"date_id\", \"symbol_id\"], how=\"left\")\n    preds_nn = np.zeros((test.shape[0],))   # Neural Network predictions\n\n    # Generate Neural Network predictions\n    # Prepare input data\n    test_input = test[CONFIG.ae_mlp_cols].to_pandas()\n    # Handle missing values: forward fill then fill remaining with zeros\n    test_input = test_input.fillna(method='ffill').fillna(0)\n    test_input = scaler.transform(test_input.values)\n    # Convert to PyTorch tensor and move to GPU\n    test_input = torch.FloatTensor(test_input).to(\"cuda:0\")\n\n    # Generate predictions from Neural Network ensemble\n    with torch.no_grad():  # Disable gradient calculation for inference\n        ae_model.eval()  # Set model to evaluation mode\n        # Average predictions from all models\n        preds_nn = ae_model(test_input).cpu().numpy().flatten()\n\n\n    # Create final predictions DataFrame\n    predictions_nn = test.select('row_id').\\\n        with_columns(\n            pl.Series(\n                name='responder_6',\n                values=np.clip(preds_nn, a_min=-5, a_max=5),  # Clip predictions to [-5, 5] range\n                dtype=pl.Float64,\n            )\n        )\n\n    return predictions_nn","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-10T09:53:40.170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_from_dill(model_name, model_path=None, file_ext='.dill'):\n    \"\"\"\n    Load a model from a dill file\n    \n    Args:\n        model_name: Name of the model file (without extension)\n        model_path: Directory path containing the model file\n        file_ext: File extension (default: '.dill')\n        \n    Returns:\n        Loaded model object\n    \"\"\"\n    model_object = None\n    # Open and load the model file using dill\n    with open(f\"{model_path}/{model_name}{file_ext}\", \"rb\") as file_handle:\n        model_object = dill.load(file_handle)\n    return model_object\n\n# Load pre-trained Ridge Regression model\nrdg = load_from_dill(\n    model_name='Ridge', \n    model_path=ridge_model_path\n)\n\ndef predict_ridge(test, lags):\n    \"\"\"\n    Make predictions using Ridge Regression model\n    \n    Args:\n        test: DataFrame containing test data\n        lags: DataFrame containing lagged features (unused in this function)\n        \n    Returns:\n        DataFrame with predictions\n    \"\"\"\n    # Select the 79 numerical features\n    cols = [f'feature_{i:02}' for i in range(79)]\n\n    # Initialize predictions DataFrame with row_id and placeholder predictions\n    predictions = test.select(\n        'row_id',\n        pl.lit(0.0).alias('responder_6'),\n    )\n\n    # Generate predictions:\n    # 1. Select required features\n    # 2. Convert to pandas\n    # 3. Fill missing values with 3\n    # 4. Make predictions using Ridge model\n    test_preds = rdg.predict(test[cols].to_pandas().fillna(3).values)\n\n    # Add predictions to result DataFrame\n    predictions = predictions.with_columns(pl.Series('responder_6', test_preds.ravel()))\n\n    return predictions","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-10T09:53:40.170Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble notebook","metadata":{}},{"cell_type":"code","source":"def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n    \"\"\" \n    Args:\n        test: DataFrame containing test data\n        lags: DataFrame containing lagged features\n        \n    Returns:\n        DataFrame with final ensemble predictions\n    \"\"\"\n    # Get predictions from each model/ensemble\n\n    pd_ae_mlp = predict_nn_ae(test, lags).to_pandas() \n    pd_nn_xgb = predict_nn_xgb(test, lags).to_pandas()     # Neural Network + XGBoost ensemble\n    pd_ridge = predict_ridge(test, lags).to_pandas()        # Ridge Regression\n    # pd_tabm = predict_tabm(test, lags).to_pandas()  \n\n    \n\n    # Rename prediction columns to avoid conflicts\n    pd_nn_xgb = pd_nn_xgb.rename(columns={'responder_6': 'col_nn_xgb'})\n    pd_ridge = pd_ridge.rename(columns={'responder_6': 'col_ridge'})\n    # pd_tabm  = pd_tabm.rename(columns={'responder_6': 'col_tabm'})\n    pd_ae_mlp  = pd_ae_mlp.rename(columns={'responder_6': 'col_ae_mlp'})\n\n    # Merge all predictions based on row_id\n    pds = pd.merge(pd_nn_xgb, pd_ridge, on=['row_id'])\n    # pds = pd.merge(pds, pd_tabm, on=['row_id'])\n    pds = pd.merge(pds, pd_ae_mlp, on=['row_id'])\n\n    e_weights = [0.60, 0.10, 0.30, 0.5] \n    # Create final weighted ensemble prediction:\n    pds['responder_6'] = (\n        pds['col_nn_xgb'] * e_weights[0] +\n        pds['col_ridge'] * e_weights[1] +\n        #pds['col_tabm'] * e_weights[2]  +\n        pds['col_ae_mlp'] * e_weights[3]\n    )\n\n    # Create final predictions DataFrame in required format\n    predictions = test.select('row_id', pl.lit(0.0).alias('responder_6'))\n    pred = pds['responder_6'].to_numpy()\n    predictions = predictions.with_columns(pl.Series('responder_6', pred.ravel()))\n\n    return predictions","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"papermill":{"duration":0.351292,"end_time":"2024-10-26T03:27:42.101707","exception":false,"start_time":"2024-10-26T03:27:41.750415","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-01-10T09:53:40.170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kaggle_evaluation.jane_street_inference_server\ninference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n        )\n    )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-10T09:53:40.170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}