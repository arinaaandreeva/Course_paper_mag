{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":214068676,"sourceType":"kernelVersion"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Here we use AE-MLP model with all lags\n\nhidden_dim =128  and lr= 1-e6, those parameters were suggested in the discusiion as the best and we also tested other parameters, which are close to those\n\nthe model is trained and saved as /kaggle/input/ae_mlp_v3/pytorch/ae_mlp_v3/1/ae_mlp_model (1).pth (https://www.kaggle.com/models/peach785/ae_mlp_v3)","metadata":{}},{"cell_type":"markdown","source":"AE для выделения более унифицированных признаков из данных. у нас все признаки в разном масштабе и разного распределения, поэтому AE может быть ключевым элементом. Перед подачей данных в AE мы их стандартизируем для приведения к одному масштабу, что может также улучшить качество модели.\n\nПосле того как данные были закодированы, они проходят через декодер, который служит для предсказания таргета, что часто улучшает качество предсказания.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\nimport numpy as np\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, WeightedRandomSampler, TensorDataset\nfrom sklearn.preprocessing import RobustScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:02:27.819859Z","iopub.execute_input":"2025-01-07T14:02:27.820355Z","iopub.status.idle":"2025-01-07T14:02:32.937047Z","shell.execute_reply.started":"2025-01-07T14:02:27.820309Z","shell.execute_reply":"2025-01-07T14:02:32.936061Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# AE-MLP with Dropout & L2-regulirization\nclass AE_MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim=128, output_dim=1, dropout_rate=0.3):\n        super(AE_MLP, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),  # Dropout after activation not to overfit\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim // 2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n# Several loss function to choose better one\n# Взвешенная Huber Loss\ndef weighted_loss(predictions, targets, weights, delta=1.0):\n    loss = nn.SmoothL1Loss(beta=delta, reduction='none')  # Huber Loss\n    per_sample_loss = loss(predictions, targets)\n    weighted_loss = (per_sample_loss * weights).mean()  # weight loss\n    return weighted_loss\n\n# # RMSE\n# def weighted_loss(predictions, targets, weights):\n#     per_sample_loss = (predictions - targets) ** 2\n#     weighted_mse = (per_sample_loss * weights).mean()\n#     weighted_rmse = torch.sqrt(weighted_mse)\n#     return weighted_rmse\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:02:35.998030Z","iopub.execute_input":"2025-01-07T14:02:35.998577Z","iopub.status.idle":"2025-01-07T14:02:36.006839Z","shell.execute_reply.started":"2025-01-07T14:02:35.998547Z","shell.execute_reply":"2025-01-07T14:02:36.005900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-3, l2_lambda=1e-5):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)  # L2-регуляризация через weight_decay\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for features, targets, weights in train_loader:\n            features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(features)\n            loss = weighted_loss(outputs.squeeze(), targets.squeeze(), weights)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Ограничение градиентов\n            optimizer.step()\n            train_loss += loss.item()\n        \n        # Валидация\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for features, targets, weights in val_loader:\n                features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n                outputs = model(features)\n                loss = weighted_loss(outputs.squeeze(), targets.squeeze(), weights)\n                val_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.6f}, Val Loss: {val_loss/len(val_loader):.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:02:39.256697Z","iopub.execute_input":"2025-01-07T14:02:39.257075Z","iopub.status.idle":"2025-01-07T14:02:39.265669Z","shell.execute_reply.started":"2025-01-07T14:02:39.257039Z","shell.execute_reply":"2025-01-07T14:02:39.264519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_dataloader(X, y, weights, batch_size=1024):\n    dataset = TensorDataset(X, y, weights)\n    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n    return loader\n\n# Ensemble mechanism\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super(EnsembleModel, self).__init__()\n        self.models = nn.ModuleList(models)\n    \n    def forward(self, x):\n        outputs = [model(x) for model in self.models]\n        return torch.stack(outputs, dim=0).mean(dim=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:02:40.871493Z","iopub.execute_input":"2025-01-07T14:02:40.871866Z","iopub.status.idle":"2025-01-07T14:02:40.878608Z","shell.execute_reply.started":"2025-01-07T14:02:40.871835Z","shell.execute_reply":"2025-01-07T14:02:40.877045Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"class CONFIG:\n    target_col = \"responder_6\"\n    lag_cols_original = [\"date_id\", \"symbol_id\"] + [f\"responder_{idx}\" for idx in range(9)]\n    lag_cols_rename = { f\"responder_{idx}\" : f\"responder_{idx}_lag_1\" for idx in range(9)}\n    valid_ratio = 0.09\n    start_dt = 1450\n\n\ntrain_path = \"/kaggle/input/Preprocessing/training.parquet/\"\nvalid_path = \"/kaggle/input/Preprocessing/validation.parquet/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:02:41.411142Z","iopub.execute_input":"2025-01-07T14:02:41.411528Z","iopub.status.idle":"2025-01-07T14:02:41.416658Z","shell.execute_reply.started":"2025-01-07T14:02:41.411492Z","shell.execute_reply":"2025-01-07T14:02:41.415578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use last 2 parquets\ntrain = pl.scan_parquet(\n    train_path\n).select(\n    pl.int_range(pl.len(), dtype=pl.UInt32).alias(\"id\"),\n    pl.all(),\n).with_columns(\n    (pl.col(CONFIG.target_col)*2).cast(pl.Int32).alias(\"label\"),\n).filter(\n    pl.col(\"date_id\").gt(CONFIG.start_dt)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:02:41.417927Z","iopub.execute_input":"2025-01-07T14:02:41.418208Z","iopub.status.idle":"2025-01-07T14:02:41.469570Z","shell.execute_reply.started":"2025-01-07T14:02:41.418186Z","shell.execute_reply":"2025-01-07T14:02:41.468135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lags = train.select(pl.col(CONFIG.lag_cols_original))\nlags = lags.rename(CONFIG.lag_cols_rename)\nlags = lags.with_columns(\n    date_id = pl.col('date_id') + 1,  # lagged by 1 day\n    )\nlags = lags.group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last()  # pick up last record of previous date\nlags","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:02:43.213704Z","iopub.execute_input":"2025-01-07T14:02:43.214110Z","iopub.status.idle":"2025-01-07T14:02:43.606827Z","shell.execute_reply.started":"2025-01-07T14:02:43.214083Z","shell.execute_reply":"2025-01-07T14:02:43.605690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = train.join(lags, on=[\"date_id\", \"symbol_id\"],  how=\"left\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:02:44.829972Z","iopub.execute_input":"2025-01-07T14:02:44.830362Z","iopub.status.idle":"2025-01-07T14:02:44.836834Z","shell.execute_reply.started":"2025-01-07T14:02:44.830333Z","shell.execute_reply":"2025-01-07T14:02:44.835504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len_train   = train.select(pl.col(\"date_id\")).collect().shape[0]\nvalid_records = int(len_train * CONFIG.valid_ratio)\nlen_ofl_mdl = len_train - valid_records\nlast_tr_dt  = train.select(pl.col(\"date_id\")).collect().row(len_ofl_mdl)[0]\n\nprint(f\"\\n len_train = {len_train}\")\nprint(f\"\\n len_ofl_mdl = {len_ofl_mdl}\")\nprint(f\"\\n---> Last offline train date = {last_tr_dt}\\n\")\n\ntraining_data = train.filter(pl.col(\"date_id\").le(last_tr_dt))\nvalidation_data   = train.filter(pl.col(\"date_id\").gt(last_tr_dt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:02:45.268674Z","iopub.execute_input":"2025-01-07T14:02:45.269067Z","iopub.status.idle":"2025-01-07T14:02:46.842578Z","shell.execute_reply.started":"2025-01-07T14:02:45.269034Z","shell.execute_reply":"2025-01-07T14:02:46.841546Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training and validating","metadata":{}},{"cell_type":"code","source":"# RobustScaler because in finantial data fat tails are common\nscaler = RobustScaler()\n\n# transforming X_train with RobustScaler\n    # select features\nX_train = training_data.select([f\"feature_{idx:02d}\" for idx in range(79)] + ['symbol_id'] + [f\"responder_{idx}\" for idx in range(9)]).collect().to_numpy().astype('float32')\n  # fill nan with mean\ncol_means = np.nanmean(X_train, axis=0)\nX_train[np.isnan(X_train)] = np.take(col_means, np.where(np.isnan(X_train))[1])\n    # fit Robust Scaling\nX_train = scaler.fit_transform(X_train)  \n\n    # select weights and target as numpy array\ny_train = training_data.select('responder_6').collect().to_numpy().astype('float32')\nweights = training_data.select('weight').collect().to_numpy().astype('float32').flatten()\n\n# transform X_val with RobustScaler\n    # select features\nX_val = validation_data.select([f\"feature_{idx:02d}\" for idx in range(79)] + ['symbol_id'] + [f\"responder_{idx}\" for idx in range(9)]).collect().to_numpy().astype('float32')\n  # fill nan with mean\ncol_means = np.nanmean(X_val, axis=0)\nX_val[np.isnan(X_val)] = np.take(col_means, np.where(np.isnan(X_val))[1])\n  # use Robust Scaling\nX_val = scaler.transform(X_val)\n\n    # select weights and target as numpy array\ny_val = validation_data.select('responder_6').collect().to_numpy().astype('float32')\nweights_val = validation_data.select('weight').collect().to_numpy().astype('float32').flatten()\n\n# Convert numpy arrays to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\nweights_train_tensor = torch.tensor(weights, dtype=torch.float32)\n\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.float32)\nweights_val_tensor = torch.tensor(weights_val, dtype=torch.float32)\n\n# Prepare DataLoaders\ntrain_loader = prepare_dataloader(X_train_tensor, y_train_tensor, weights_train_tensor)\nval_loader = prepare_dataloader(X_val_tensor, y_val_tensor, weights_val_tensor)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:16:48.927398Z","iopub.execute_input":"2025-01-07T12:16:48.927725Z","iopub.status.idle":"2025-01-07T12:17:53.168107Z","shell.execute_reply.started":"2025-01-07T12:16:48.927699Z","shell.execute_reply":"2025-01-07T12:17:53.165931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model, optimizer, and start training\n# With validation\ninput_dim = X_train_tensor.shape[1]  # Number of features\nmodel = AE_MLP(input_dim=input_dim, hidden_dim=128)\n\ntrain_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:17:53.170651Z","iopub.execute_input":"2025-01-07T12:17:53.171101Z","iopub.status.idle":"2025-01-07T13:02:33.544053Z","shell.execute_reply.started":"2025-01-07T12:17:53.171044Z","shell.execute_reply":"2025-01-07T13:02:33.542857Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hubert Loss Weighted R2: 0.999055\nRMSE Weighted R2: 0.999034\n","metadata":{}},{"cell_type":"markdown","source":"Count R2","metadata":{}},{"cell_type":"code","source":"def weighted_r2_on_batches(val_loader, model):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    \n    weighted_sum = 0.0\n    weighted_mean_sum = 0.0\n    ss_residual = 0.0\n    ss_total = 0.0\n\n    with torch.no_grad():\n        weights_sum = 0.0\n        \n        for features, targets, weights in val_loader:\n            features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n            \n            preds = model(features)\n            weights_sum += weights.sum().item()\n            \n            # Вычисление взвешенного среднего на GPU\n            batch_weighted_mean = (weights * targets).sum()\n            weighted_sum += batch_weighted_mean.item()\n            weighted_mean_sum += (weights * targets).sum().item()\n\n            # Вычисление отклонений\n            ss_residual += (weights * (targets - preds) ** 2).sum().item()\n            ss_total += (weights * (targets - (batch_weighted_mean / weights.sum())) ** 2).sum().item()\n\n        # Общий взвешенный средний\n        weighted_mean = weighted_sum / weights_sum\n\n    # Итоговый R²\n    r2 = 1 - (ss_residual / ss_total)\n    return r2\n\n# Подсчет R²\nr2_score = weighted_r2_on_batches(val_loader, model)\nprint(f\"Weighted R2: {r2_score:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:02:50.990645Z","iopub.execute_input":"2025-01-07T14:02:50.991005Z","iopub.status.idle":"2025-01-07T14:02:51.056042Z","shell.execute_reply.started":"2025-01-07T14:02:50.990977Z","shell.execute_reply":"2025-01-07T14:02:51.054587Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## train on all data","metadata":{}},{"cell_type":"code","source":"# All data for training final model\nX_data = train.select(\n    [f\"feature_{idx:02d}\" for idx in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)] + ['symbol_id', 'time_id', 'date_id']\n).collect().to_numpy().astype('float32')\n\ncol_means = np.nanmean(X_data, axis=0) # fill nan with mean\nX_data[np.isnan(X_data)] = np.take(col_means, np.where(np.isnan(X_data))[1])\ny_data = train.select('responder_6').collect().to_numpy().astype('float32')\nall_weights = train.select('weight').collect().to_numpy().astype('float32').flatten()\n\n\n# scaling data\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_data)\n\n# Convert to torch tensors\nX_data_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\ny_data_tensor = torch.tensor(y_data, dtype=torch.float32)\nall_weights_tensor = torch.tensor(all_weights, dtype=torch.float32)\n\n# # Save the scaler for future use\nimport joblib\njoblib.dump(scaler, 'robust_scaler_07_01.pkl')\n\n\nall_train_loader = prepare_dataloader(X_data_tensor, y_data_tensor, all_weights_tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:03:24.827104Z","iopub.execute_input":"2025-01-07T14:03:24.827497Z","iopub.status.idle":"2025-01-07T14:04:28.665929Z","shell.execute_reply.started":"2025-01-07T14:03:24.827425Z","shell.execute_reply":"2025-01-07T14:04:28.664061Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"for prediction:\n\n Load the saved scaler\nscaler = joblib.load('robust_scaler.pkl')\n\n Transform input data for prediction\nX_test_scaled = scaler.transform(X_test)\n\n Convert to torch tensors\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n\n Perform prediction\nwith torch.no_grad():\n    model.eval()\n    X_test_tensor = X_test_tensor.to(device)\n    predictions = model(X_test_tensor).cpu().numpy()\n\n Optionally, inverse transform the predictions\npredictions_original_scale = scaler.inverse_transform(predictions)\n\n","metadata":{}},{"cell_type":"code","source":"# train.select(\n#     [f\"feature_{idx:02d}\" for idx in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)] + ['symbol_id', 'time_id', 'date_id']\n# ).collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T03:38:17.332628Z","iopub.status.idle":"2025-01-06T03:38:17.333037Z","shell.execute_reply":"2025-01-06T03:38:17.332849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_on_full_data(model, full_loader, num_epochs=10, learning_rate=1e-3, l2_lambda=1e-5):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)  # L2-регуляризация через weight_decay\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for features, targets, weights in full_loader:\n            features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(features)\n            loss = weighted_loss(outputs.squeeze(), targets.squeeze(), weights)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Ограничение градиентов\n            optimizer.step()\n            train_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(full_loader):.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:04:28.668468Z","iopub.execute_input":"2025-01-07T14:04:28.668932Z","iopub.status.idle":"2025-01-07T14:04:28.679098Z","shell.execute_reply.started":"2025-01-07T14:04:28.668890Z","shell.execute_reply":"2025-01-07T14:04:28.677792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model, optimizer, and start training\ninput_dim = X_data.shape[1]  # Number of features\nmodel = AE_MLP(input_dim=input_dim, hidden_dim=128)\n\ntrain_on_full_data(model, all_train_loader, num_epochs=20, learning_rate=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T04:31:18.246204Z","iopub.execute_input":"2025-01-06T04:31:18.246575Z","iopub.status.idle":"2025-01-06T06:01:59.044972Z","shell.execute_reply.started":"2025-01-06T04:31:18.246550Z","shell.execute_reply":"2025-01-06T06:01:59.043492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Сохранение модели\nmodel_save_path = \"ae_mlp_model_06_01_2025.pth\"\ntorch.save(model.state_dict(), model_save_path)\n\nprint(f\"Model saved at {model_save_path}\")\n\n# import joblib\n# joblib.dump(model.state_dict(), 'ae_mlp_04_01_2025.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T06:01:59.046923Z","iopub.execute_input":"2025-01-06T06:01:59.047470Z","iopub.status.idle":"2025-01-06T06:01:59.066834Z","shell.execute_reply.started":"2025-01-06T06:01:59.047436Z","shell.execute_reply":"2025-01-06T06:01:59.065216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 128 learning_rate=1e-5\n# Epoch 1/10, Train Loss: 0.688642, Val Loss: 0.482744\n# Epoch 2/10, Train Loss: 0.684686, Val Loss: 0.484899\n\n# 256\n# Epoch 1/10, Train Loss: 0.686451, Val Loss: 0.484307\n# Epoch 2/10, Train Loss: 0.685009, Val Loss: 0.486779\n\n# 128 learning_rate=1e-6\n# Epoch 1/10, Train Loss: 0.842413, Val Loss: 0.487302\n# Epoch 2/10, Train Loss: 0.688747, Val Loss: 0.484387\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}