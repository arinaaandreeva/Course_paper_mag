{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":11305158,"sourceType":"competition"},{"sourceId":214068676,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport polars as pl\nimport pandas as pd\nimport catboost as cbt\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import TimeSeriesSplit\nimport matplotlib.pyplot as plt\nimport gc\nimport warnings\n\n# Настройки\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    target = \"responder_6\"\n    features = [f\"feature_{i:02d}\" for i in range(79)] \n    lags = [f\"responder_{i}_lag_1\" for i in range(9)]\n    cat_features = [\"symbol_id\"]\n    val_size = 0.1\n    n_splits = 2  # Можно уменьшить до 1 для максимальной экономии памяти\n\ndef load_data():\n    \"\"\"Оптимизированная загрузка данных\"\"\"\n    columns = Config.features + Config.lags + Config.cat_features + [Config.target, \"weight\"]\n    return (\n        pl.scan_parquet(\"/kaggle/input/js24-preprocessing-catboost/training.parquet/\")\n        .select(columns)\n    )\n\ndef prepare_features(data):\n    \"\"\"Подготовка фичей с контролем памяти\"\"\"\n    df = data.collect(streaming=True)\n    # Переводим все фичи в float32 для экономии памяти\n    X = df[Config.features + Config.lags + Config.cat_features].to_pandas()\n    X = X.astype({col: np.float32 for col in Config.features + Config.lags})\n    y = df[Config.target].to_numpy().astype(np.float32)\n    weights = df[\"weight\"].to_numpy().astype(np.float32)\n\n    del df\n    gc.collect()\n    for col in X.columns:\n        if X[col].isnull().any():\n            X[col].fillna(X[col].mean(), inplace=True)\n    cat_col_idx = [X.columns.get_loc(col) for col in Config.cat_features]\n    X[Config.cat_features] = X[Config.cat_features].astype('category')\n    return X, y, weights, cat_col_idx\n\ndef weighted_r2(y_true, y_pred, weights):\n    \"\"\"Эффективная реализация взвешенной R2\"\"\"\n    weights = np.ones_like(y_true) if weights is None else weights\n    y_mean = np.average(y_true, weights=weights)\n    numerator = np.sum(weights * (y_true - y_pred)**2)\n    denominator = np.sum(weights * (y_true - y_mean)**2)\n    return 1 - numerator / (denominator + 1e-10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_xgboost(X, y, weights, n_splits=2):\n    \"\"\"Обучение XGBoost\"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    results = []\n\n    # One-hot encoding для категориального признака\n    cat_col = Config.cat_features[0]\n    X_encoded = pd.get_dummies(X, columns=[cat_col], dtype=np.float32)\n\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_encoded), 1):\n        print(f\"\\nXGBoost Fold {fold}/{n_splits}\")\n\n        X_train, X_val = X_encoded.iloc[train_idx], X_encoded.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        w_train, w_val = weights[train_idx], weights[val_idx]\n\n        # Выравнивание колонок\n        X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n        X_train = X_train.astype(np.float32)\n        X_val = X_val.astype(np.float32)\n\n        model = xgb.XGBRegressor(\n            objective='reg:squarederror',\n            n_estimators=30,\n            max_depth=3,\n            subsample=0.5,\n            colsample_bytree=0.5,\n            min_child_weight=10,\n            tree_method='gpu_hist',\n            predictor='gpu_predictor',\n            eval_metric='rmse',\n            n_jobs=1\n        )\n\n        model.fit(\n            X_train, y_train,\n            sample_weight=w_train,\n            eval_set=[(X_train, y_train), (X_val, y_val)],\n            sample_weight_eval_set=[w_train, w_val],\n            verbose=100,\n            early_stopping_rounds=10\n        )\n\n        score = weighted_r2(y_val, model.predict(X_val), w_val)\n        results.append(score)\n        print(f\"XGBoost Fold {fold} R2: {score:.4f}\")\n\n        # Очистка памяти\n        del model, X_train, X_val, y_train, y_val, w_train, w_val\n        gc.collect()\n\n    del X_encoded\n    gc.collect()\n    return results\n\ndef train_lightgbm(X, y, weights, cat_col_idx, n_splits=2):\n    \"\"\"Обучение LightGBM\"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    results = []\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n        print(f\"\\nLightGBM Fold {fold}/{n_splits}\")\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        w_train, w_val = weights[train_idx], weights[val_idx]\n        # Используем float32\n        X_train = X_train.astype(np.float32)\n        X_val = X_val.astype(np.float32)\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=30,      # Меньше деревьев\n            learning_rate=0.05,\n            max_depth=3,\n            num_leaves=15,\n            min_child_samples=50,\n            subsample=0.5,\n            colsample_bytree=0.5,\n            device='gpu',\n            verbose=-1\n        )\n        model.fit(\n            X_train, y_train,\n            sample_weight=w_train,\n            eval_set=[(X_val, y_val)],\n            eval_sample_weight=[w_val],\n            eval_metric='rmse',\n            categorical_feature=cat_col_idx,\n            callbacks=[lgb.early_stopping(10), lgb.log_evaluation(20)]\n        )\n        score = weighted_r2(y_val, model.predict(X_val), w_val)\n        results.append(score)\n        print(f\"LightGBM Fold {fold} R2: {score:.4f}\")\n        del model, X_train, X_val, y_train, y_val, w_train, w_val\n        gc.collect()\n    return results\n\ndef train_catboost(X, y, weights, cat_col_idx, n_splits=2):\n    \"\"\"Обучение CatBoost\"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    results = []\n    \n    # Преобразование категориального признака\n    X_cb = X.copy()\n    cat_col = X.columns[cat_col_idx[0]]  \n    X_cb[cat_col] = X_cb[cat_col].astype(int).astype(str)\n    \n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_cb), 1):\n        print(f\"\\nCatBoost Fold {fold}/{n_splits}\")\n        \n        X_train, X_val = X_cb.iloc[train_idx], X_cb.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        w_train, w_val = weights[train_idx], weights[val_idx]\n        \n        model = cbt.CatBoostRegressor(\n            iterations=300,\n            learning_rate=0.05,\n            loss_function='RMSE',\n            eval_metric='R2',\n            early_stopping_rounds=20,\n            cat_features=cat_col_idx,\n            task_type='GPU',\n            verbose=100\n        )\n        \n        model.fit(\n            X_train, y_train,\n            sample_weight=w_train,\n            eval_set=(X_val, y_val)\n        )\n        \n        score = weighted_r2(y_val, model.predict(X_val), w_val)\n        results.append(score)\n        print(f\"CatBoost Fold {fold} R2: {score:.4f}\")\n        \n        del model, X_train, X_val, y_train, y_val, w_train, w_val\n        gc.collect()\n    \n    return results\n\n\n\n\"\"\"Сравнение моделей\"\"\"\n# print(\"Loading data...\")\ndata = load_data()\nX, y, weights, cat_col_idx = prepare_features(data)\ndel data\ngc.collect()\nprint(\"\\n=== Training LightGBM ===\")\nlgb_results = train_lightgbm(X, y, weights, cat_col_idx, n_splits=2)\nprint(\"\\n=== Training CatBoost ===\")\ncb_results = train_catboost(X, y, weights, cat_col_idx, n_splits=2)\nprint(\"\\n=== Training XGBoost ===\")\nxgb_results = train_xgboost(X, y, weights)\n\nresults = {\n    \"XGBoost\": xgb_results,\n    \"LightGBM\": lgb_results,\n    \"CatBoost\": cb_results\n}\n# plt.figure(figsize=(10, 5))\n# colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n# for (model, scores), color in zip(results.items(), colors):\n#     plt.plot(range(1, len(scores)+1), scores, \n#             label=model, marker='o', linewidth=2, color=color)\n# plt.title(\"Model Comparison by Fold (Weighted R2)\", fontsize=14)\n# plt.xlabel(\"Fold Number\", fontsize=12)\n# plt.ylabel(\"Weighted R2 Score\", fontsize=12)\n# plt.legend(fontsize=12)\n# plt.grid(True, linestyle='--', alpha=0.7)\n# plt.xticks(range(1, 2))\n# plt.tight_layout()\n# plt.show()\n# print(\"\\n=== Final Results ===\")\n# final_scores = {}\n# for model, scores in results.items():\n#     mean_score = np.mean(scores)\n#     std_score = np.std(scores)\n#     final_scores[model] = mean_score\n#     print(f\"{model}:\")\n#     print(f\"  Mean R2: {mean_score:.4f}\")\n#     print(f\"  Std R2: {std_score:.4f}\")\n#     print(f\"  Fold scores: {[round(s, 4) for s in scores]}\")\n# best_model = max(final_scores.items(), key=lambda x: x[1])\n# print(f\"\\nRecommendation: Use {best_model[0]} as baseline (highest mean R2: {best_model[1]:.4f})\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T01:15:53.534837Z","iopub.execute_input":"2025-06-21T01:15:53.535041Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n\n=== Training LightGBM ===\n\nLightGBM Fold 1/2\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 10 rounds\n[20]\tvalid_0's rmse: 0.813913\tvalid_0's l2: 0.662455\nDid not meet early stopping. Best iteration is:\n[30]\tvalid_0's rmse: 0.813848\tvalid_0's l2: 0.662349\nLightGBM Fold 1 R2: 0.0020\n\nLightGBM Fold 2/2\nTraining until validation scores don't improve for 10 rounds\n[20]\tvalid_0's rmse: 0.788171\tvalid_0's l2: 0.621214\nDid not meet early stopping. Best iteration is:\n[30]\tvalid_0's rmse: 0.78795\tvalid_0's l2: 0.620865\nLightGBM Fold 2 R2: 0.0025\n\n=== Training CatBoost ===\n\nCatBoost Fold 1/2\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because R2 is/are not implemented for GPU\nMetric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 0.0012309\ttest: 0.0001904\tbest: 0.0001904 (0)\ttotal: 298ms\tremaining: 1m 29s\nbestTest = 0.002406201858\nbestIteration = 69\nShrink model to first 70 iterations.\nCatBoost Fold 1 R2: 0.0025\n\nCatBoost Fold 2/2\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because R2 is/are not implemented for GPU\nMetric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 0.0006887\ttest: 0.0001955\tbest: 0.0001955 (0)\ttotal: 368ms\tremaining: 1m 50s\n100:\tlearn: 0.0299593\ttest: 0.0044574\tbest: 0.0044574 (100)\ttotal: 28.7s\tremaining: 56.6s\n200:\tlearn: 0.0478854\ttest: 0.0050300\tbest: 0.0050300 (200)\ttotal: 56.1s\tremaining: 27.7s\nbestTest = 0.005265070133\nbestIteration = 267\nShrink model to first 268 iterations.\nCatBoost Fold 2 R2: 0.0052\n\n=== Training XGBoost ===\n\nXGBoost Fold 1/2\n[0]\tvalidation_0-rmse:0.74941\tvalidation_1-rmse:0.81468\n[19]\tvalidation_0-rmse:0.73676\tvalidation_1-rmse:0.81441\nXGBoost Fold 1 R2: 0.0020\n\nXGBoost Fold 2/2\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"=== Training XGBoost ===\n\nXGBoost Fold 1/2\n[0]\tvalidation_0-rmse:0.74941\tvalidation_1-rmse:0.81468\n[18]\tvalidation_0-rmse:0.73809\tvalidation_1-rmse:0.81446\nXGBoost Fold 1 R2: 0.0020\n\nXGBoost Fold 2/2\n[0]\tvalidation_0-rmse:0.78023\tvalidation_1-rmse:0.78867\n[26]\tvalidation_0-rmse:0.77247\tvalidation_1-rmse:0.78769\nXGBoost Fold 2 R2: 0.0035\n\n=== Training CatBoost ===\n\n0:\tlearn: 0.0012309\ttest: 0.0001904\tbest: 0.0001904 (0)\ttotal: 299ms\tremaining: 1m 29s\nbestTest = 0.002406201858\nbestIteration = 69\nShrink model to first 70 iterations.\nCatBoost Fold 1 R2: 0.0025\n\nCatBoost Fold 2/2\nDefault metric period is 5 because R2 is/are not implemented for GPU\nMetric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n0:\tlearn: 0.0006887\ttest: 0.0001955\tbest: 0.0001955 (0)\ttotal: 370ms\tremaining: 1m 50s\n100:\tlearn: 0.0299593\ttest: 0.0044574\tbest: 0.0044574 (100)\ttotal: 28.6s\tremaining: 56.4s\n200:\tlearn: 0.0478854\ttest: 0.0050300\tbest: 0.0050300 (200)\ttotal: 56s\tremaining: 27.6s\nbestTest = 0.005265070133\nbestIteration = 267\nShrink model to first 268 iterations.\nCatBoost Fold 2 R2: 0.0052\n\n\n=== Training LightGBM ===\n\nTraining until validation scores don't improve for 10 rounds\n[20]\tvalid_0's rmse: 0.813913\tvalid_0's l2: 0.662455\nDid not meet early stopping. Best iteration is:\n[30]\tvalid_0's rmse: 0.813848\tvalid_0's l2: 0.662349\nLightGBM Fold 1 R2: 0.0020\n\nLightGBM Fold 2/2\nTraining until validation scores don't improve for 10 rounds\n[20]\tvalid_0's rmse: 0.788171\tvalid_0's l2: 0.621214\nDid not meet early stopping. Best iteration is:\n[30]\tvalid_0's rmse: 0.78795\tvalid_0's l2: 0.620865\nLightGBM Fold 2 R2: 0.0025","metadata":{}}]}