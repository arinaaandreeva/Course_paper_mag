{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84493,"databundleVersionId":11305158,"sourceType":"competition"},{"sourceId":214068676,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport polars as pl\nimport pandas as pd\nimport catboost as cbt\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import TimeSeriesSplit\nimport matplotlib.pyplot as plt\nimport gc\nimport warnings\n\n# Настройки\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T08:44:04.104356Z","iopub.execute_input":"2025-06-22T08:44:04.104592Z","iopub.status.idle":"2025-06-22T08:44:11.856479Z","shell.execute_reply.started":"2025-06-22T08:44:04.104574Z","shell.execute_reply":"2025-06-22T08:44:11.855677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    target = \"responder_6\"\n    features = [f\"feature_{i:02d}\" for i in range(79)] \n    lags = [f\"responder_{i}_lag_1\" for i in range(9)]\n    cat_features = [\"symbol_id\"]\n    val_size = 0.1\n    n_splits = 2  # Можно уменьшить до 1 для максимальной экономии памяти\n\ntrain_path = \"/kaggle/input/Preprocessing/training.parquet/\"\nvalid_path = \"/kaggle/input/Preprocessing/validation.parquet/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data():\n    \"\"\"Оптимизированная загрузка данных\"\"\"\n    columns = Config.features + Config.lags + Config.cat_features + [Config.target, \"weight\"]\n    return (\n        pl.scan_parquet(train_path)\n        .select(columns)\n    )\n\ndef prepare_features(data):\n    \"\"\"Подготовка фичей с контролем памяти\"\"\"\n    df = data.collect(streaming=True)\n    # Переводим все фичи в float32 для экономии памяти\n    X = df[Config.features + Config.lags + Config.cat_features].to_pandas()\n    X = X.astype({col: np.float32 for col in Config.features + Config.lags})\n    y = df[Config.target].to_numpy().astype(np.float32)\n    weights = df[\"weight\"].to_numpy().astype(np.float32)\n\n    del df\n    gc.collect()\n    for col in X.columns:\n        if X[col].isnull().any():\n            X[col].fillna(X[col].mean(), inplace=True)\n    cat_col_idx = [X.columns.get_loc(col) for col in Config.cat_features]\n    X[Config.cat_features] = X[Config.cat_features].astype('category')\n    return X, y, weights, cat_col_idx\n\ndef weighted_r2(y_true, y_pred, weights):\n    \"\"\"Эффективная реализация взвешенной R2\"\"\"\n    weights = np.ones_like(y_true) if weights is None else weights\n    y_mean = np.average(y_true, weights=weights)\n    numerator = np.sum(weights * (y_true - y_pred)**2)\n    denominator = np.sum(weights * (y_true - y_mean)**2)\n    return 1 - numerator / (denominator + 1e-10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T08:44:11.858019Z","iopub.execute_input":"2025-06-22T08:44:11.858794Z","iopub.status.idle":"2025-06-22T08:44:11.867373Z","shell.execute_reply.started":"2025-06-22T08:44:11.858755Z","shell.execute_reply":"2025-06-22T08:44:11.866672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_xgboost(X, y, weights, n_splits=3):\n    \"\"\"Обучение XGBoost с визуализацией реальных и предсказанных значений на каждом фолде\"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    results = []\n\n    cat_col = Config.cat_features[0]\n    X_encoded = pd.get_dummies(X, columns=[cat_col], dtype=np.float32)\n\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_encoded), 1):\n        print(f\"\\nXGBoost Fold {fold}/{n_splits}\")\n\n        X_train, X_val = X_encoded.iloc[train_idx], X_encoded.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        w_train, w_val = weights[train_idx], weights[val_idx]\n\n        X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n        X_train = X_train.astype(np.float32)\n        X_val = X_val.astype(np.float32)\n\n        model = xgb.XGBRegressor(\n            objective='reg:squarederror',\n            learning_rate=0.05,\n            n_estimators=200,\n            max_depth=3,\n            subsample=0.5,\n            colsample_bytree=0.5,\n            min_child_weight=10,\n            tree_method='gpu_hist',\n            predictor='gpu_predictor',\n            eval_metric='rmse',\n            n_jobs=1\n        )\n\n        model.fit(\n            X_train, y_train,\n            sample_weight=w_train,\n            eval_set=[(X_train, y_train), (X_val, y_val)],\n            sample_weight_eval_set=[w_train, w_val],\n            verbose=100,\n            early_stopping_rounds=10\n        )\n\n        y_pred = model.predict(X_val)\n        score = weighted_r2(y_val, y_pred, w_val)\n        results.append(score)\n        print(f\"XGBoost Fold {fold} R2: {score:.4f}\")\n\n        # Визуализация реальных и предсказанных значений\n        plt.figure(figsize=(12, 5))\n        plt.plot(y_val, label='Real', alpha=0.7)\n        plt.plot(y_pred, label='Predicted', alpha=0.7)\n        plt.title(f'Fold {fold} - Real vs Predicted')\n        plt.xlabel('Sample index')\n        plt.ylabel('Target value')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n        del model, X_train, X_val, y_train, y_val, w_train, w_val\n        gc.collect()\n\n    del X_encoded\n    gc.collect()\n    return results\n\ndef train_lightgbm(X, y, weights, cat_col_idx, n_splits=3):\n    \"\"\"Обучение LightGBM\"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    results = []\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n        print(f\"\\nLightGBM Fold {fold}/{n_splits}\")\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        w_train, w_val = weights[train_idx], weights[val_idx]\n        # Используем float32\n        X_train = X_train.astype(np.float32)\n        X_val = X_val.astype(np.float32)\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=200,      # Меньше деревьев\n            learning_rate=0.05,\n            max_depth=3,\n            num_leaves=15,\n            min_child_samples=50,\n            subsample=0.5,\n            colsample_bytree=0.5,\n            device='gpu',\n            verbose=-1\n        )\n        model.fit(\n            X_train, y_train,\n            sample_weight=w_train,\n            eval_set=[(X_val, y_val)],\n            eval_sample_weight=[w_val],\n            eval_metric='rmse',\n            categorical_feature=cat_col_idx,\n            callbacks=[lgb.early_stopping(10), lgb.log_evaluation(20)]\n        )\n        score = weighted_r2(y_val, model.predict(X_val), w_val)\n        results.append(score)\n        print(f\"LightGBM Fold {fold} R2: {score:.4f}\")\n        del model, X_train, X_val, y_train, y_val, w_train, w_val\n        gc.collect()\n    return results\n\ndef train_catboost(X, y, weights, cat_col_idx, n_splits=3):\n    \"\"\"Обучение CatBoost\"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    results = []\n    \n    # Преобразование категориального признака\n    X_cb = X.copy()\n    cat_col = X.columns[cat_col_idx[0]]  \n    X_cb[cat_col] = X_cb[cat_col].astype(int).astype(str)\n    \n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_cb), 1):\n        print(f\"\\nCatBoost Fold {fold}/{n_splits}\")\n        \n        X_train, X_val = X_cb.iloc[train_idx], X_cb.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        w_train, w_val = weights[train_idx], weights[val_idx]\n        \n        model = cbt.CatBoostRegressor(\n            iterations=200,\n            learning_rate=0.05,\n            loss_function='RMSE',\n            eval_metric='R2',\n            early_stopping_rounds=20,\n            cat_features=cat_col_idx,\n            task_type='GPU',\n            verbose=100\n        )\n        \n        model.fit(\n            X_train, y_train,\n            sample_weight=w_train,\n            eval_set=(X_val, y_val)\n        )\n        \n        score = weighted_r2(y_val, model.predict(X_val), w_val)\n        results.append(score)\n        print(f\"CatBoost Fold {fold} R2: {score:.4f}\")\n        \n        del model, X_train, X_val, y_train, y_val, w_train, w_val\n        gc.collect()\n    \n    return results\n\n\n\n\"\"\"Сравнение моделей\"\"\"\n# print(\"Loading data...\")\ndata = load_data()\nX, y, weights, cat_col_idx = prepare_features(data)\ndel data\ngc.collect()\nprint(\"\\n=== Training LightGBM ===\")\nlgb_results = train_lightgbm(X, y, weights, cat_col_idx, n_splits=3)\nprint(\"\\n=== Training CatBoost ===\")\ncb_results = train_catboost(X, y, weights, cat_col_idx, n_splits=3)\nprint(\"\\n=== Training XGBoost ===\")\nxgb_results = train_xgboost(X, y, weights)\n\nresults = {\n    \"XGBoost\": xgb_results,\n    \"LightGBM\": lgb_results,\n    \"CatBoost\": cb_results\n}\n","metadata":{"execution":{"iopub.status.busy":"2025-06-22T08:44:22.109236Z","iopub.execute_input":"2025-06-22T08:44:22.109848Z","iopub.status.idle":"2025-06-22T08:56:42.977035Z","shell.execute_reply.started":"2025-06-22T08:44:22.109824Z","shell.execute_reply":"2025-06-22T08:56:42.976267Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c']\nfor (model, scores), color in zip(results.items(), colors):\n    plt.plot(range(1, len(scores)+1), scores, \n            label=model, marker='o', linewidth=2, color=color)\nplt.title(\"Model Comparison by Fold (Weighted R2)\", fontsize=14)\nplt.xlabel(\"Fold Number\", fontsize=12)\nplt.ylabel(\"Weighted R2 Score\", fontsize=12)\nplt.legend(fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.xticks(range(1, 2))\nplt.tight_layout()\nplt.show()\nprint(\"\\n=== Final Results ===\")\nfinal_scores = {}\nfor model, scores in results.items():\n    mean_score = np.mean(scores)\n    std_score = np.std(scores)\n    final_scores[model] = mean_score\n    print(f\"{model}:\")\n    print(f\"  Mean R2: {mean_score:.4f}\")\n    print(f\"  Std R2: {std_score:.4f}\")\n    print(f\"  Fold scores: {[round(s, 4) for s in scores]}\")\nbest_model = max(final_scores.items(), key=lambda x: x[1])\nprint(f\"\\nRecommendation: Use {best_model[0]} as baseline (highest mean R2: {best_model[1]:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T08:56:42.978121Z","iopub.execute_input":"2025-06-22T08:56:42.978391Z","iopub.status.idle":"2025-06-22T08:56:43.184495Z","shell.execute_reply.started":"2025-06-22T08:56:42.978368Z","shell.execute_reply":"2025-06-22T08:56:43.183797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}