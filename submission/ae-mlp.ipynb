{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":214068676,"sourceType":"kernelVersion"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Here we use AE-MLP model with all lags\n\nhidden_dim =128  and lr= 1-e6, those parameters were suggested in the discusiion as the best and we also tested other parameters, which are close to those\n\nthe model is trained and saved as /kaggle/input/ae_mlp_v3/pytorch/ae_mlp_v3/1/ae_mlp_model (1).pth, however the code for submition has some mistakes, which i need to solve in order to submit ","metadata":{}},{"cell_type":"markdown","source":"AE для выделения более унифицированных признаков из данных. у нас все признаки в разном масштабе и разного распределения, поэтому AE может быть ключевым элементом. Перед подачей данных в AE мы их стандартизируем для приведения к одному масштабу, что может также улучшить качество модели.\n\nПосле того как данные были закодированы, они проходят через декодер, который служит для предсказания таргета, что часто улучшает качество предсказания.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\nimport numpy as np\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, WeightedRandomSampler, TensorDataset\nfrom sklearn.preprocessing import RobustScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:16:32.965085Z","iopub.execute_input":"2025-01-07T12:16:32.965835Z","iopub.status.idle":"2025-01-07T12:16:39.143396Z","shell.execute_reply.started":"2025-01-07T12:16:32.965785Z","shell.execute_reply":"2025-01-07T12:16:39.142161Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# AE-MLP with Dropout & L2-regulirization\nclass AE_MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim=128, output_dim=1, dropout_rate=0.3):\n        super(AE_MLP, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),  # Dropout after activation not to overfit\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim // 2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n# Several loss function to choose better one\n# Взвешенная Huber Loss\ndef weighted_loss(predictions, targets, weights, delta=1.0):\n    loss = nn.SmoothL1Loss(beta=delta, reduction='none')  # Huber Loss\n    per_sample_loss = loss(predictions, targets)\n    weighted_loss = (per_sample_loss * weights).mean()  # weight loss\n    return weighted_loss\n\n# # RMSE\n# def weighted_loss(predictions, targets, weights):\n#     per_sample_loss = (predictions - targets) ** 2\n#     weighted_mse = (per_sample_loss * weights).mean()\n#     weighted_rmse = torch.sqrt(weighted_mse)\n#     return weighted_rmse\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:16:39.144740Z","iopub.execute_input":"2025-01-07T12:16:39.145206Z","iopub.status.idle":"2025-01-07T12:16:39.153829Z","shell.execute_reply.started":"2025-01-07T12:16:39.145174Z","shell.execute_reply":"2025-01-07T12:16:39.152199Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-3, l2_lambda=1e-5):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)  # L2-регуляризация через weight_decay\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for features, targets, weights in train_loader:\n            features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(features)\n            loss = weighted_loss(outputs.squeeze(), targets.squeeze(), weights)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Ограничение градиентов\n            optimizer.step()\n            train_loss += loss.item()\n        \n        # Валидация\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for features, targets, weights in val_loader:\n                features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n                outputs = model(features)\n                loss = weighted_loss(outputs.squeeze(), targets.squeeze(), weights)\n                val_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.6f}, Val Loss: {val_loss/len(val_loader):.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:16:39.156866Z","iopub.execute_input":"2025-01-07T12:16:39.157213Z","iopub.status.idle":"2025-01-07T12:16:39.189119Z","shell.execute_reply.started":"2025-01-07T12:16:39.157187Z","shell.execute_reply":"2025-01-07T12:16:39.187633Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def prepare_dataloader(X, y, weights, batch_size=1024):\n    dataset = TensorDataset(X, y, weights)\n    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n    return loader\n\n# Ensemble mechanism\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super(EnsembleModel, self).__init__()\n        self.models = nn.ModuleList(models)\n    \n    def forward(self, x):\n        outputs = [model(x) for model in self.models]\n        return torch.stack(outputs, dim=0).mean(dim=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:16:39.190897Z","iopub.execute_input":"2025-01-07T12:16:39.191279Z","iopub.status.idle":"2025-01-07T12:16:39.214899Z","shell.execute_reply.started":"2025-01-07T12:16:39.191250Z","shell.execute_reply":"2025-01-07T12:16:39.213005Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"class CONFIG:\n    target_col = \"responder_6\"\n    lag_cols_original = [\"date_id\", \"symbol_id\"] + [f\"responder_{idx}\" for idx in range(9)]\n    lag_cols_rename = { f\"responder_{idx}\" : f\"responder_{idx}_lag_1\" for idx in range(9)}\n    valid_ratio = 0.09\n    start_dt = 1450","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:16:39.216609Z","iopub.execute_input":"2025-01-07T12:16:39.217375Z","iopub.status.idle":"2025-01-07T12:16:39.252878Z","shell.execute_reply.started":"2025-01-07T12:16:39.217300Z","shell.execute_reply":"2025-01-07T12:16:39.251416Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Use last 2 parquets\ntrain = pl.scan_parquet(\n    f\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet\"\n).select(\n    pl.int_range(pl.len(), dtype=pl.UInt32).alias(\"id\"),\n    pl.all(),\n).with_columns(\n    (pl.col(CONFIG.target_col)*2).cast(pl.Int32).alias(\"label\"),\n).filter(\n    pl.col(\"date_id\").gt(CONFIG.start_dt)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:16:42.693565Z","iopub.execute_input":"2025-01-07T12:16:42.694001Z","iopub.status.idle":"2025-01-07T12:16:42.747206Z","shell.execute_reply.started":"2025-01-07T12:16:42.693966Z","shell.execute_reply":"2025-01-07T12:16:42.745687Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"lags = train.select(pl.col(CONFIG.lag_cols_original))\nlags = lags.rename(CONFIG.lag_cols_rename)\nlags = lags.with_columns(\n    date_id = pl.col('date_id') + 1,  # lagged by 1 day\n    )\nlags = lags.group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last()  # pick up last record of previous date\nlags","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:16:42.901623Z","iopub.execute_input":"2025-01-07T12:16:42.901960Z","iopub.status.idle":"2025-01-07T12:16:43.330802Z","shell.execute_reply.started":"2025-01-07T12:16:42.901931Z","shell.execute_reply":"2025-01-07T12:16:43.328754Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<LazyFrame at 0x7CB518ECDCC0>","text/html":"<h4>NAIVE QUERY PLAN</h4><p>run <b>LazyFrame.show_graph()</b> to see the optimized version</p><?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: polars_query Pages: 1 -->\n<svg width=\"1679pt\" height=\"567pt\"\n viewBox=\"0.00 0.00 1679.00 567.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 563)\">\n<title>polars_query</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-563 1675,-563 1675,4 -4,4\"/>\n<!-- p1 -->\n<g id=\"node1\" class=\"node\">\n<title>p1</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1671,-559 0,-559 0,-506 1671,-506 1671,-559\"/>\n<text text-anchor=\"middle\" x=\"835.5\" y=\"-543.8\" font-family=\"Times,serif\" font-size=\"14.00\">AGG [col(&quot;responder_0_lag_1&quot;).last(), col(&quot;responder_1_lag_1&quot;).last(), col(&quot;responder_2_lag_1&quot;).last(), col(&quot;responder_3_lag_1&quot;).last(), col(&quot;responder_4_lag_1&quot;).last(), col(&quot;responder_5_lag_1&quot;).last(), col(&quot;responder_6_lag_1&quot;).last(), col(&quot;responder_7_lag_1&quot;).last(), col(&quot;responder_8_lag_1&quot;).last()]</text>\n<text text-anchor=\"middle\" x=\"835.5\" y=\"-528.8\" font-family=\"Times,serif\" font-size=\"14.00\">BY</text>\n<text text-anchor=\"middle\" x=\"835.5\" y=\"-513.8\" font-family=\"Times,serif\" font-size=\"14.00\">[col(&quot;date_id&quot;), col(&quot;symbol_id&quot;)]</text>\n</g>\n<!-- p2 -->\n<g id=\"node2\" class=\"node\">\n<title>p2</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1011.5,-470 659.5,-470 659.5,-434 1011.5,-434 1011.5,-470\"/>\n<text text-anchor=\"middle\" x=\"835.5\" y=\"-448.3\" font-family=\"Times,serif\" font-size=\"14.00\">WITH COLUMNS [[(col(&quot;date_id&quot;)) + (1)].alias(&quot;date_id&quot;)]</text>\n</g>\n<!-- p1&#45;&#45;p2 -->\n<g id=\"edge1\" class=\"edge\">\n<title>p1&#45;&#45;p2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M835.5,-505.8C835.5,-494.17 835.5,-480.74 835.5,-470.33\"/>\n</g>\n<!-- p3 -->\n<g id=\"node3\" class=\"node\">\n<title>p3</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"872.5,-398 798.5,-398 798.5,-362 872.5,-362 872.5,-398\"/>\n<text text-anchor=\"middle\" x=\"835.5\" y=\"-376.3\" font-family=\"Times,serif\" font-size=\"14.00\">RENAME</text>\n</g>\n<!-- p2&#45;&#45;p3 -->\n<g id=\"edge2\" class=\"edge\">\n<title>p2&#45;&#45;p3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M835.5,-433.7C835.5,-422.85 835.5,-408.92 835.5,-398.1\"/>\n</g>\n<!-- p4 -->\n<g id=\"node4\" class=\"node\">\n<title>p4</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"864.5,-326 806.5,-326 806.5,-290 864.5,-290 864.5,-326\"/>\n<text text-anchor=\"middle\" x=\"835.5\" y=\"-304.3\" font-family=\"Times,serif\" font-size=\"14.00\">π 11/11</text>\n</g>\n<!-- p3&#45;&#45;p4 -->\n<g id=\"edge3\" class=\"edge\">\n<title>p3&#45;&#45;p4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M835.5,-361.7C835.5,-350.85 835.5,-336.92 835.5,-326.1\"/>\n</g>\n<!-- p5 -->\n<g id=\"node5\" class=\"node\">\n<title>p5</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"953,-254 718,-254 718,-218 953,-218 953,-254\"/>\n<text text-anchor=\"middle\" x=\"835.5\" y=\"-232.3\" font-family=\"Times,serif\" font-size=\"14.00\">FILTER BY [(col(&quot;date_id&quot;)) &gt; (1450)]</text>\n</g>\n<!-- p4&#45;&#45;p5 -->\n<g id=\"edge4\" class=\"edge\">\n<title>p4&#45;&#45;p5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M835.5,-289.7C835.5,-278.85 835.5,-264.92 835.5,-254.1\"/>\n</g>\n<!-- p6 -->\n<g id=\"node6\" class=\"node\">\n<title>p6</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1070.5,-182 600.5,-182 600.5,-146 1070.5,-146 1070.5,-182\"/>\n<text text-anchor=\"middle\" x=\"835.5\" y=\"-160.3\" font-family=\"Times,serif\" font-size=\"14.00\">WITH COLUMNS [[(col(&quot;responder_6&quot;)) * (2.0)].strict_cast(Int32).alias(&quot;label&quot;)]</text>\n</g>\n<!-- p5&#45;&#45;p6 -->\n<g id=\"edge5\" class=\"edge\">\n<title>p5&#45;&#45;p6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M835.5,-217.7C835.5,-206.85 835.5,-192.92 835.5,-182.1\"/>\n</g>\n<!-- p7 -->\n<g id=\"node7\" class=\"node\">\n<title>p7</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"864.5,-110 806.5,-110 806.5,-74 864.5,-74 864.5,-110\"/>\n<text text-anchor=\"middle\" x=\"835.5\" y=\"-88.3\" font-family=\"Times,serif\" font-size=\"14.00\">π 94/94</text>\n</g>\n<!-- p6&#45;&#45;p7 -->\n<g id=\"edge6\" class=\"edge\">\n<title>p6&#45;&#45;p7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M835.5,-145.7C835.5,-134.85 835.5,-120.92 835.5,-110.1\"/>\n</g>\n<!-- p8 -->\n<g id=\"node8\" class=\"node\">\n<title>p8</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1211.5,-38 459.5,-38 459.5,0 1211.5,0 1211.5,-38\"/>\n<text text-anchor=\"middle\" x=\"835.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">Parquet SCAN [/kaggle/input/jane&#45;street&#45;real&#45;time&#45;market&#45;data&#45;forecasting/train.parquet/partition_id=0/part&#45;0.parquet, ... 9 other files]</text>\n<text text-anchor=\"middle\" x=\"835.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">π */93;</text>\n</g>\n<!-- p7&#45;&#45;p8 -->\n<g id=\"edge7\" class=\"edge\">\n<title>p7&#45;&#45;p8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M835.5,-73.81C835.5,-62.98 835.5,-49.01 835.5,-38.02\"/>\n</g>\n</g>\n</svg>\n"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train = train.join(lags, on=[\"date_id\", \"symbol_id\"],  how=\"left\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:16:46.079724Z","iopub.execute_input":"2025-01-07T12:16:46.080111Z","iopub.status.idle":"2025-01-07T12:16:46.086384Z","shell.execute_reply.started":"2025-01-07T12:16:46.080081Z","shell.execute_reply":"2025-01-07T12:16:46.084855Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"len_train   = train.select(pl.col(\"date_id\")).collect().shape[0]\nvalid_records = int(len_train * CONFIG.valid_ratio)\nlen_ofl_mdl = len_train - valid_records\nlast_tr_dt  = train.select(pl.col(\"date_id\")).collect().row(len_ofl_mdl)[0]\n\nprint(f\"\\n len_train = {len_train}\")\nprint(f\"\\n len_ofl_mdl = {len_ofl_mdl}\")\nprint(f\"\\n---> Last offline train date = {last_tr_dt}\\n\")\n\ntraining_data = train.filter(pl.col(\"date_id\").le(last_tr_dt))\nvalidation_data   = train.filter(pl.col(\"date_id\").gt(last_tr_dt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:16:46.214766Z","iopub.execute_input":"2025-01-07T12:16:46.215308Z","iopub.status.idle":"2025-01-07T12:16:47.930266Z","shell.execute_reply.started":"2025-01-07T12:16:46.215265Z","shell.execute_reply":"2025-01-07T12:16:47.929010Z"}},"outputs":[{"name":"stdout","text":"\n len_train = 9144696\n\n len_ofl_mdl = 8321674\n\n---> Last offline train date = 1677\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Training and validating","metadata":{}},{"cell_type":"code","source":"# RobustScaler because in finantial data fat tails are common\nscaler = RobustScaler()\n\n# transforming X_train with RobustScaler\n    # select features\nX_train = training_data.select([f\"feature_{idx:02d}\" for idx in range(79)] + ['symbol_id'] + [f\"responder_{idx}\" for idx in range(9)]).collect().to_numpy().astype('float32')\n  # fill nan with mean\ncol_means = np.nanmean(X_train, axis=0)\nX_train[np.isnan(X_train)] = np.take(col_means, np.where(np.isnan(X_train))[1])\n    # fit Robust Scaling\nX_train = scaler.fit_transform(X_train)  \n\n    # select weights and target as numpy array\ny_train = training_data.select('responder_6').collect().to_numpy().astype('float32')\nweights = training_data.select('weight').collect().to_numpy().astype('float32').flatten()\n\n# transform X_val with RobustScaler\n    # select features\nX_val = validation_data.select([f\"feature_{idx:02d}\" for idx in range(79)] + ['symbol_id'] + [f\"responder_{idx}\" for idx in range(9)]).collect().to_numpy().astype('float32')\n  # fill nan with mean\ncol_means = np.nanmean(X_val, axis=0)\nX_val[np.isnan(X_val)] = np.take(col_means, np.where(np.isnan(X_val))[1])\n  # use Robust Scaling\nX_val = scaler.transform(X_val)\n\n    # select weights and target as numpy array\ny_val = validation_data.select('responder_6').collect().to_numpy().astype('float32')\nweights_val = validation_data.select('weight').collect().to_numpy().astype('float32').flatten()\n\n# Convert numpy arrays to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\nweights_train_tensor = torch.tensor(weights, dtype=torch.float32)\n\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.float32)\nweights_val_tensor = torch.tensor(weights_val, dtype=torch.float32)\n\n# Prepare DataLoaders\ntrain_loader = prepare_dataloader(X_train_tensor, y_train_tensor, weights_train_tensor)\nval_loader = prepare_dataloader(X_val_tensor, y_val_tensor, weights_val_tensor)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:16:48.927398Z","iopub.execute_input":"2025-01-07T12:16:48.927725Z","iopub.status.idle":"2025-01-07T12:17:53.168107Z","shell.execute_reply.started":"2025-01-07T12:16:48.927699Z","shell.execute_reply":"2025-01-07T12:17:53.165931Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Initialize model, optimizer, and start training\n# With validation\ninput_dim = X_train_tensor.shape[1]  # Number of features\nmodel = AE_MLP(input_dim=input_dim, hidden_dim=128)\n\ntrain_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:17:53.170651Z","iopub.execute_input":"2025-01-07T12:17:53.171101Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hubert Loss Weighted R2: 0.999055\nRMSE Weighted R2: 0.999034\n","metadata":{}},{"cell_type":"markdown","source":"Count R2","metadata":{}},{"cell_type":"code","source":"def weighted_r2_on_batches(val_loader, model):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    \n    weighted_sum = 0.0\n    weighted_mean_sum = 0.0\n    ss_residual = 0.0\n    ss_total = 0.0\n\n    with torch.no_grad():\n        weights_sum = 0.0\n        \n        for features, targets, weights in val_loader:\n            features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n            \n            preds = model(features)\n            weights_sum += weights.sum().item()\n            \n            # Вычисление взвешенного среднего на GPU\n            batch_weighted_mean = (weights * targets).sum()\n            weighted_sum += batch_weighted_mean.item()\n            weighted_mean_sum += (weights * targets).sum().item()\n\n            # Вычисление отклонений\n            ss_residual += (weights * (targets - preds) ** 2).sum().item()\n            ss_total += (weights * (targets - (batch_weighted_mean / weights.sum())) ** 2).sum().item()\n\n        # Общий взвешенный средний\n        weighted_mean = weighted_sum / weights_sum\n\n    # Итоговый R²\n    r2 = 1 - (ss_residual / ss_total)\n    return r2\n\n# Подсчет R²\nr2_score = weighted_r2_on_batches(val_loader, model)\nprint(f\"Weighted R2: {r2_score:.6f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## train on all data","metadata":{}},{"cell_type":"code","source":"# All data for training final model\nX_data = train.select(\n    [f\"feature_{idx:02d}\" for idx in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)] + ['symbol_id', 'time_id', 'date_id']\n).collect().to_numpy().astype('float32')\n\ncol_means = np.nanmean(X_data, axis=0) # fill nan with mean\nX_data[np.isnan(X_data)] = np.take(col_means, np.where(np.isnan(X_data))[1])\ny_data = train.select('responder_6').collect().to_numpy().astype('float32')\nall_weights = train.select('weight').collect().to_numpy().astype('float32').flatten()\n\n\n# scaling data\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_data)\n\n# Convert to torch tensors\nX_data_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\ny_data_tensor = torch.tensor(y_data, dtype=torch.float32)\nall_weights_tensor = torch.tensor(all_weights, dtype=torch.float32)\n\n# # Save the scaler for future use\n# import joblib\n# joblib.dump(scaler, 'robust_scaler.pkl')\n\n\nall_train_loader = prepare_dataloader(X_data_tensor, y_data_tensor, all_weights_tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T04:30:00.619202Z","iopub.execute_input":"2025-01-06T04:30:00.619879Z","iopub.status.idle":"2025-01-06T04:30:55.511457Z","shell.execute_reply.started":"2025-01-06T04:30:00.619827Z","shell.execute_reply":"2025-01-06T04:30:55.510257Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"for prediction:\n\n Load the saved scaler\nscaler = joblib.load('robust_scaler.pkl')\n\n Transform input data for prediction\nX_test_scaled = scaler.transform(X_test)\n\n Convert to torch tensors\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n\n Perform prediction\nwith torch.no_grad():\n    model.eval()\n    X_test_tensor = X_test_tensor.to(device)\n    predictions = model(X_test_tensor).cpu().numpy()\n\n Optionally, inverse transform the predictions\npredictions_original_scale = scaler.inverse_transform(predictions)\n\n","metadata":{}},{"cell_type":"code","source":"# train.select(\n#     [f\"feature_{idx:02d}\" for idx in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)] + ['symbol_id', 'time_id', 'date_id']\n# ).collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T03:38:17.332628Z","iopub.status.idle":"2025-01-06T03:38:17.333037Z","shell.execute_reply":"2025-01-06T03:38:17.332849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_on_full_data(model, full_loader, num_epochs=10, learning_rate=1e-3, l2_lambda=1e-5):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)  # L2-регуляризация через weight_decay\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for features, targets, weights in full_loader:\n            features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(features)\n            loss = weighted_loss(outputs.squeeze(), targets.squeeze(), weights)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Ограничение градиентов\n            optimizer.step()\n            train_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(full_loader):.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T04:31:12.859262Z","iopub.execute_input":"2025-01-06T04:31:12.859650Z","iopub.status.idle":"2025-01-06T04:31:12.866938Z","shell.execute_reply.started":"2025-01-06T04:31:12.859610Z","shell.execute_reply":"2025-01-06T04:31:12.865498Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Initialize model, optimizer, and start training\ninput_dim = X_data.shape[1]  # Number of features\nmodel = AE_MLP(input_dim=input_dim, hidden_dim=128)\n\ntrain_on_full_data(model, all_train_loader, num_epochs=20, learning_rate=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T04:31:18.246204Z","iopub.execute_input":"2025-01-06T04:31:18.246575Z","iopub.status.idle":"2025-01-06T06:01:59.044972Z","shell.execute_reply.started":"2025-01-06T04:31:18.246550Z","shell.execute_reply":"2025-01-06T06:01:59.043492Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20, Train Loss: 0.683378\nEpoch 2/20, Train Loss: 0.679475\nEpoch 3/20, Train Loss: 0.677553\nEpoch 4/20, Train Loss: 0.676222\nEpoch 5/20, Train Loss: 0.676512\nEpoch 6/20, Train Loss: 0.675851\nEpoch 7/20, Train Loss: 0.675880\nEpoch 8/20, Train Loss: 0.675583\nEpoch 9/20, Train Loss: 0.674823\nEpoch 10/20, Train Loss: 0.674470\nEpoch 11/20, Train Loss: 0.674284\nEpoch 12/20, Train Loss: 0.673810\nEpoch 13/20, Train Loss: 0.674095\nEpoch 14/20, Train Loss: 0.672962\nEpoch 15/20, Train Loss: 0.673624\nEpoch 16/20, Train Loss: 0.673771\nEpoch 17/20, Train Loss: 0.673186\nEpoch 18/20, Train Loss: 0.672427\nEpoch 19/20, Train Loss: 0.673470\nEpoch 20/20, Train Loss: 0.673083\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\n\n# Сохранение модели\nmodel_save_path = \"ae_mlp_model_06_01_2025.pth\"\ntorch.save(model.state_dict(), model_save_path)\n\nprint(f\"Model saved at {model_save_path}\")\n\n# import joblib\n# joblib.dump(model.state_dict(), 'ae_mlp_04_01_2025.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T06:01:59.046923Z","iopub.execute_input":"2025-01-06T06:01:59.047470Z","iopub.status.idle":"2025-01-06T06:01:59.066834Z","shell.execute_reply.started":"2025-01-06T06:01:59.047436Z","shell.execute_reply":"2025-01-06T06:01:59.065216Z"}},"outputs":[{"name":"stdout","text":"Model saved at ae_mlp_model_06_01_2025.pth\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['ae_mlp_04_01_2025.pkl']"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# 128 learning_rate=1e-5\n# Epoch 1/10, Train Loss: 0.688642, Val Loss: 0.482744\n# Epoch 2/10, Train Loss: 0.684686, Val Loss: 0.484899\n\n# 256\n# Epoch 1/10, Train Loss: 0.686451, Val Loss: 0.484307\n# Epoch 2/10, Train Loss: 0.685009, Val Loss: 0.486779\n\n# 128 learning_rate=1e-6\n# Epoch 1/10, Train Loss: 0.842413, Val Loss: 0.487302\n# Epoch 2/10, Train Loss: 0.688747, Val Loss: 0.484387\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
